<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning">
  <meta name="keywords" content="Robot Learning, Retrieval, Dynamic Time Warping, Vision Foundation Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XHJWDHYL2Z"></script>
  <script>

    // Google Analytics
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-XHJWDHYL2Z');

    // Libero task widget
    VIDEO_PATH = './static/videos/retrieval_strips/';
    TASK_NAMES_LIBERO = [
      'pick_up_the_book_and_place_it_in_the_back_compartment_of_the_caddy',
      'put_both_moka_pots_on_the_stove',
      'put_both_the_alphabet_soup_and_the_cream_cheese_box_in_the_basket',
      'put_both_the_alphabet_soup_and_the_tomato_sauce_in_the_basket',
      'put_both_the_cream_cheese_box_and_the_butter_in_the_basket',
      'put_the_black_bowl_in_the_bottom_drawer_of_the_cabinet_and_close_it',
      'put_the_white_mug_on_the_left_plate_and_put_the_yellow_and_white_mug_on_the_right_plate',
      'put_the_white_mug_on_the_plate_and_put_the_chocolate_pudding_to_the_right_of_the_plate',
      'put_the_yellow_and_white_mug_in_the_microwave_and_close_it',
      'turn_on_the_stove_and_put_the_moka_pot_on_it'
    ];
    TASK_INSTRUCTION_LIBERO = [
      '"pick up the book and place it in the back compartment of the caddy"',
      '"put both moka pots on the stove"',
      '"put both the alphabet soup and the cream cheese box in the basket"',
      '"put both the alphabet soup and the tomato sauce in the basket"',
      '"put both the cream cheese box and the butter in the basket"',
      '"put the black bowl in the bottom drawer of the cabinet and close it"',
      '"put the white mug on the left plate and put the yellow and white mug on the right plate"',
      '"put the white mug on the plate and put the chocolate pudding to the right of the plate"',
      '"put the yellow and white mug in the microwave and close it"',
      '"turn on the stove and put the moka pot on it'
    ];
    TASK_N_SUB_LIBERO = [
      4,
      9,
      7,
      10,
      4,
      4,
      7,
      6,
      6,
      6
    ];

    var libero_vid_idx = 0;
    function mod(n, m) {
      return ((n % m) + m) % m;
    }

    function prevLiberoTask() {
      libero_vid_idx = mod(libero_vid_idx - 1, TASK_NAMES_LIBERO.length);
      setLiberoTask(libero_vid_idx);
    }

    function nextLiberoTask() {
      libero_vid_idx = (libero_vid_idx + 1) % TASK_NAMES_LIBERO.length;
      setLiberoTask(libero_vid_idx);
    }

    function setLiberoTask(libero_vid_idx) {

      var task_str = TASK_NAMES_LIBERO[libero_vid_idx];

      for (var i = 0; i < 10; i++) {

        if (i >= TASK_N_SUB_LIBERO[libero_vid_idx]) {
          // Empty the video element
          $('#task-libero-vid-input-' + String(i)).empty();
          continue;
        }

        let path = VIDEO_PATH + task_str + '/segment_' + String(i) + '/video_strip.mp4';

        // Create the video element
        let video = document.createElement('video');
        video.id = 'replay-video';    // Set an ID if needed
        video.controls = false;        // Enable controls
        video.autoplay = true;        // Autoplay the video
        video.playsinline = true;     // For inline playback
        video.loop = true;            // Loop the video
        video.muted = true;           // Mute the video

        // Create the source element
        let source = document.createElement('source');
        source.src = path;
        source.type = 'video/mp4';    // Set the correct video type

        // Append the source to the video element
        video.appendChild(source);

        // Append the video to the target element
        let input_id = '#task-libero-vid-input-' + String(i);
        video.onloadeddata = function () {
          $(input_id).empty().append(video);
        };
      };

      document.getElementById('task-libero-string').innerHTML = '<i>' + TASK_INSTRUCTION_LIBERO[libero_vid_idx] + '</i>';
    }

    TASK_NAMES_REAL = [
      'pepper_3',
      'chili_3',
      'carrot_3'
    ];
    TASK_INSTRUCTION_REAL = [
      '"pick up the pepper ü´ë and put it in the sink üö∞"',
      '"pick up the chili üå∂Ô∏è and put it in the pan üç≥"',
      '"pick up the carrot ü•ï and put it on the plate üçΩÔ∏è"'
    ];
    TASK_N_SUB_REAL = [
      2, //3,
      2,
      2
    ];

    var real_vid_idx = 0;
    function mod(n, m) {
      return ((n % m) + m) % m;
    }

    function prevRealTask() {
      real_vid_idx = mod(real_vid_idx - 1, TASK_NAMES_REAL.length);
      setRealTask(real_vid_idx);
    }

    function nextRealTask() {
      real_vid_idx = (real_vid_idx + 1) % TASK_NAMES_REAL.length;
      setRealTask(real_vid_idx);
    }

    function setRealTask(real_vid_idx) {

      var task_str = TASK_NAMES_REAL[real_vid_idx];

      for (var i = 0; i < 10; i++) {

        if (i >= TASK_N_SUB_REAL[real_vid_idx]) {
          // Empty the video element
          $('#task-real-vid-input-' + String(i)).empty();
          continue;
        }

        let path = VIDEO_PATH + task_str + '/segment_' + String(i) + '/video_strip.mp4';

        // Create the video element
        let video = document.createElement('video');
        video.id = 'replay-video';    // Set an ID if needed
        video.controls = false;        // Enable controls
        video.autoplay = true;        // Autoplay the video
        video.playsinline = true;     // For inline playback
        video.loop = true;            // Loop the video
        video.muted = true;           // Mute the video

        // Create the source element
        let source = document.createElement('source');
        source.src = path;
        source.type = 'video/mp4';    // Set the correct video type

        // Append the source to the video element
        video.appendChild(source);

        // Append the video to the target element
        let input_id = '#task-real-vid-input-' + String(i);
        video.onloadeddata = function () {
          $(input_id).empty().append(video);
        };
      };


      let path = VIDEO_PATH + task_str + '/legend.png';
      let img = document.createElement('img');
      let input_id = '#task-real-legend';

      // Set the image source -> legend
      img.src = path;
      img.style.display = 'block';
      img.style.margin = '0 auto';
      img.style.maxWidth = '70%';
      img.style.height = 'auto';
      // Use the correct onload event
      img.onload = function () {
        // Clear the container and append the new image
        $(input_id).empty().append(img);
      };
        
      document.getElementById('task-real-string').innerHTML = '<i>' + TASK_INSTRUCTION_REAL[real_vid_idx] + '</i>';
    }

    setLiberoTask(0);
    setRealTask(0);

  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- TEX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" integrity="sha384-..."
    crossorigin="anonymous"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img style="vertical-align: middle; width: 250px; height: auto;" src="./static/images/strap_logo.png"
              alt="STRAP logo" />
            <h1 class="title is-1 publication-title">Robot Sub-Trajectory Retrieval for Augmented Policy Learning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://memmelma.github.io">Marius Memmel</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jacob-b-066234203/">Jacob Berg</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=LYt_2MgAAAAJ&hl=en">Bingqing
                  Chen</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://abhishekunique.github.io/">Abhishek Gupta</a><sup>&dagger;1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7CLS0LwAAAAJ&hl=en">Jonathan
                  Francis</a><sup>&dagger;2</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Washington</span>
              <span class="author-block"><sup>2</sup>Bosch Center for Artificial Intelligence</span>
            </div>
            <div>
              <span class="author-block"><sup>*</sup>equal contribution</span>
              <span class="author-block"><sup>&dagger;</sup>equal advising</span>
            </div>

            <!-- <div class="is-size-5 has-text-weight-semibold mt-3 publication-title">
              <span class="author-block">conference placeholder</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <!-- <span class="link-block">
                  <a href="static/documents/strap_2025.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span> -->
                <span class="link-block">
                  <a href="http://arxiv.org/abs/2412.15182" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <!-- <span class="link-block">
                  <a href="YOUTUBE" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video (coming soon)</span>
                  </a>
                </span> -->
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="TBD" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

                <!-- Poster download link. -->
                <!-- <span class="link-block">
                  <a href="static/assets/CVPR2023_VOT_poster.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-sticky-note"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span> -->
                <!-- Slides download link. -->
                <!-- <span class="link-block">
                  <a href="static/assets/CVPR2023_VOT_slides.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Slides</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-centered is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline>
          <source src="./static/videos/teaser.mp4" type="video/mp4">
        </video>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">

            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p><b>tl;dr</b> <i>Retrieval</i> for robust few-shot imitation learning by encoding trajectories with
                <i>vision foundation models</i> and retrieving <i>sub-trajectories</i> with <i>subsequence dynamic time
                  warping.</i>
              </p>
              <p>
                <!-- Robot learning is witnessing a significant increase in the size, diversity, and complexity of
                pre-collected datasets, mirroring trends in domains such as natural language processing and computer
                vision. Many robot learning methods treat such datasets as multi-task expert data and learn a
                multi-task, generalist policy by training broadly across them. Notably, while these generalist policies
                can improve the average performance across many tasks, the performance of generalist policies on any one
                task is often suboptimal due to negative transfer between partitions of the data, compared to
                task-specific specialist policies. In this work, we argue for the paradigm of training policies during
                deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen
                problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant
                data at test time. Furthermore, we show that many robotics tasks share considerable amounts of low-level
                behaviors and that retrieval at the "sub"-trajectory granularity enables significantly improved data
                utilization, generalization, and robustness in adapting policies to novel problems. In contrast,
                existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared
                cross-task content. This work proposes \(\texttt{STRAP}\), a technique for leveraging pre-trained vision
                foundation
                models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in
                a robust fashion. \(\texttt{STRAP}\) outperforms both prior retrieval algorithms and multi-task learning
                methods in
                simulated and real experiments, showing the ability to scale to much larger offline datasets in the real
                world as well as the ability to learn robust control policies with just a handful of real-world
                demonstrations. -->
                Robot learning is increasingly relying on large, diverse, and complex datasets, similar to trends in NLP
                and computer vision. Generalist policies trained on these datasets can perform well across multiple
                tasks but often underperform on individual tasks due to negative data transfer. This work proposes
                training policies during deployment, adjusting to specific scenarios rather than using pre-trained,
                zero-shot models. Our approach, \(\texttt{STRAP}\), retrieves and trains on relevant data at a
                sub-trajectory level, enhancing robustness. Results show that \(\texttt{STRAP}\)
                surpasses existing methods in both simulated and real experiments, achieving robust control with minimal
                real-world demonstrations.
              </p>
            </div>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <!-- <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe width="400" src="static/videos/strap_talk_crop.mp4" title="YouTube video player" frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              allowfullscreen></iframe>
          </div>
        </div>
      </div> -->
      <!--/ Paper video. -->
    </div>
  </section>

  <!-- Pipeline Overview -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Method Overview</h2>
        </div>
      </div>
      <div class="container is-max-desktop">

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <!-- <img src="./static/images/method_figure.png" class="money-figure" alt="Pipeline Overview" /> -->
            <video autoplay muted loop playsinline width="100%">
              <source src="./static/videos/method.mov" type="video/mp4">
            </video>
            <br>
            <br>
            <p>
              <b>Overview of \(\texttt{STRAP}\):</b> 1) demonstrations \(\mathcal{D}_{target}\) and offline dataset
              \(\mathcal{D}_{prior}\) are encoded into a shared embedding space using a vision foundation model, 2)
              automatic slicing generates sub-trajectories which 3) Subsequence Dynamic Time Warping (S-DTW) matches to
              corresponding sub-trajectories in \(\mathcal{D}_{prior}\) creating \(\mathcal{D}_{retrieved}\), 4)
              training a policy on the union of \(\mathcal{D}_{retrieved}\) and \(\mathcal{D}_{target}\) results in
              better performance and robustness.
            </p>

            <br>
            <div class="content">
              <p><b>Our key insights are:</b></p>
              <ul>
                <li><strong>Vision foundation models</strong> offer powerful out-of-the-box representations for
                  trajectory retrieval. They sufficiently encode scene semantics and offer visual robustness in contrast
                  to brittle in-domain feature extractors from prior work.
                </li>
                <li><strong>Sub-trajectory retrieval</strong> enable maximal re-use of prior data capturing temporality
                  of tasks and dynamics.</li>
                <li><strong>Subsequence Dynamic Time Warping</strong> finds optimal sub-trajectory matches in offline
                  datasets agnostic to length, horizon or demonstration frequency.</li>
              </ul>
            </div>
          </div>
        </div>
      </div>
      <div class="columns is-centered is-mobile is-vcentered">

      </div>
    </div>
    </div>
    </div>
  </section>
  <!--/ Pipeline Overview -->



  <!-- Real-World Experiments -->
  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Kitchen Experiments</h2>
        </div>
      </div>
      <p>
        <h3 class="title is-5">Videos</h3>
        </p>
      <div class="columns is-mobile is-vcentered">
        <div class="column is-4">
          <div class="content has-text-centered">
            <video id="replay-video" autoplay playsinline loop muted controls>
              <source src="./static/videos/results_real/table_retrieval_3_small.mp4" type="video/mp4">
            </video>
            <p>Table</p>
          </div>
        </div>

        <div class="column is-4">
          <div class="content has-text-centered">
            <video id="replay-video" autoplay playsinline loop muted controls>
              <source src="./static/videos/results_real/sink_retrieval_3_small.mp4" type="video/mp4">
            </video>
            <p>Sink</p>
          </div>
        </div>
        <div class="column is-4">
          <div class="content has-text-centered">
            <video id="replay-video" autoplay playsinline loop muted controls>
              <source src="./static/videos/results_real/stove_retrieval_3_small.mp4" type="video/mp4">
            </video>
            <p>Stove</p>
          </div>
        </div>
      </div>
      <div class="container is-max-desktop">
        <p>
        <h3 class="title is-5">Results (Kitchen)</h3>
        </p>
        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <img src="./static/images/results_real_kitchen.png" class="money-figure" alt="Pipeline Overview"
              style="display: block; margin: 0 auto; max-width: 50%; height: auto;" />
            <br>
            <p>
              \(\texttt{STRAP}\) solves several pick and place tasks in a realistic kitchen environment.
              \(\texttt{STRAP}\) is surprisingly <b>robust to object poses unseen in demonstrations</b> while baselines
              fail to adapt. The policy further shows recovery behavior, completing the task even when the initial grasp
              fails and alters the object's pose.

            </p>
          </div>
        </div>
      </div>
      <br>
      <br>
      <p>
      <h3 class="title is-5">Results (Kitchen-DROID)</h3>
      </p>
      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
            
          <img src="./static/images/results_real_kitchen_droid.png" class="money-figure" alt="Pipeline Overview"
            style="display: block; margin: 0 auto; max-width: 50%; height: auto;" />
          <br>
          <p>
            To investigate scalability to larger datasets, we construct an additional offline dataset consisting of 5k
            demonstrations from the DROID dataset and 50 demonstrations collected in the same environment.
            Since DTW scales linearly with the number of demonstrations and \(\texttt{STRAP}\)'s policy training stage
            is independent of the offline dataset's size but depends on the amount of data retrieved, it <b>naturally
              scales to larger datasets</b> like DROID while maintaining performance.

          </p>
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <br>
      <p>
      <h3 class="title is-5">Datasets</h3>
      </p>
      <div class="columns is-mobile is-vcentered">
        <div class="column is-half">
          <div class="content has-text-centered">
            <p>Demonstration Dataset \(\mathcal{D}_{target}\)</p>
            <video id="replay-video" autoplay playsinline loop muted controls>
              <source src="./static/videos/kitchen_demos.mp4" type="video/mp4">
            </video>
            <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
          </div>
        </div>

        <div class="column is-half">
          <div class="content has-text-centered">
            <p>Offline Dataset \(\mathcal{D}_{prior}\)</p>
            <video id="replay-video" autoplay playsinline loop muted controls>
              <source src="./static/videos/kitchen_dataset.mp4" type="video/mp4">
            </video>
            <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
          </div>
        </div>
      </div>
      <br>
      <br>
      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Pen-in-Cup Experiments</h2>
        </div>
      </div>
      <p>
        <h3 class="title is-5">Videos</h3>
        </p>
         <div class="columns is-mobile is-vcentered">
          <div class="column is-2"></div>
          <div class="column is-4">
            <div class="content has-text-centered">
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/teaser/pen_in_cup_base.mp4" type="video/mp4">
              </video>
              <p>Base</p>
            </div>
          </div>
          <div class="column is-4">
            <div class="content has-text-centered">
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/teaser/pen_in_cup_ood_1.mp4" type="video/mp4">
              </video>
              <p>OOD</p>
            </div>
          </div>
          <div class="column is-2"></div>

        </div>
      <h3 class="title is-5">Results</h3>
      </p>
      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">

          <img src="./static/images/results_real.png" class="money-figure" alt="Pipeline Overview"
            style="display: block; margin: 0 auto; max-width: 50%; height: auto;" />
          <br>
          <p>
            \(\texttt{STRAP}\) can leverage data from seemingly unrelated tasks to learn robustness behavior.
            While behavior cloning replays the demonstrations, \(\texttt{STRAP}\) can adapt to unseen object poses and
            appearance.

            <!-- \(\mathcal{D}_{target}\)
              (<i>base</i>), BC lacks robustness to out-of-distribution (<i>OOD</i>) scenarios. The policy replays
              the trajectories observed in \(\mathcal{D}_{target}\). \(\texttt{STRAP}\) retrieves relevant
              sub-trajectories from \(\mathcal{D}_{prior}\), <i>e.g.</i>,
              the robot putting the screwdriver in the cup or picking up pens in various poses. Augmented policy
              learning then distills this knowledge into a policy, resulting in generalization to an OOD scenario. -->
          </p>
        </div>
      </div>
    </div>

    <!-- Intuition -->
    <div class="container is-max-desktop">

      <!-- <br>
        <p>
          <b>In-distribution (base)</b>
        </p>
        <br>
        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <div class="content has-text-centered">
              <video id="replay-video" controls autoplay playsinline loop muted>
                <source src="./static/videos/pen_in_cup_base.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>

        <div class="container is-max-desktop">
          
          <br>
          <p>
            <b>Out-of-distribution (OOD)</b>
          </p>
          <br>
          <div class="columns is-centered is-mobile is-vcentered">
            <div class="column is-full-width">
              <div class="content has-text-centered">
                <video id="replay-video" controls autoplay playsinline loop muted>
                  <source src="./static/videos/pen_in_cup_ood.mp4" type="video/mp4">
                </video>
              </div>
            </div>
          </div> -->

      <div class="container is-max-desktop">
        <!-- <h2 class="title is-4">Datasets</h2> -->
        <br>
        <p>
        <h3 class="title is-5">Datasets</h3>
        </p>
        <div class="columns is-mobile is-vcentered">
          <div class="column is-half">
            <div class="content has-text-centered">
              <p>Demonstration Dataset \(\mathcal{D}_{target}\)</p>
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/pick_dataset.mp4" type="video/mp4">
              </video>
              <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
            </div>
          </div>

          <div class="column is-half">
            <div class="content has-text-centered">
              <p>Offline Dataset \(\mathcal{D}_{prior}\)</p>
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/10x10_dataset.mp4" type="video/mp4">
              </video>
              <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
            </div>
          </div>
        </div>


      </div>
    </div>
    </div>
  </section>
  <!-- Real-World Experiments -->

  <!-- Experiments -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">LIBERO-10 Experiments</h2>
        </div>
      </div>
      <div class="container is-max-desktop">
        <p>
        <h3 class="title is-5">Results</h3>
        </p>
        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">

            <!-- <p>
              <b>Simulation Experiments:</b>
            </p>
            <br>
            <br> -->

            <img src="./static/images/results_sim.png" class="money-figure" alt="Pipeline Overview"
              style="display: block; margin: 0 auto; max-width: 90%; height: auto;" />
            <br>
            <br>
            <p>
              <b>\(\texttt{STRAP}\) outperforms the retrieval baselines</b> <i>BehaviorRetrieval</i> (BR) [Du et al.
              2023] and
              <i>FlowRetrieval</i> (FR) [Lin et al. 2024] on average by \(+24.7\%\) and \(+25.0\%\) across all
              LIBERO-10 tasks. These results demonstrate the policy's robustness to object poses. Both <b>DINOv2
                and CLIP are viable representations for \(\texttt{STRAP}\)</b> with only a
              \(+0.73\%\)
              difference across all LIBERO-10 tasks.
            </p>
          </div>
        </div>
      </div>
      <div class="container is-max-desktop">
        <br>
        <p>
        <h3 class="title is-5">Datasets</h3>
        </p>
        <div class="columns is-mobile is-vcentered">
          <div class="column is-half">
            <div class="content has-text-centered">
              <p>Demonstration Dataset \(\mathcal{D}_{target}\)</p>
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/libero10_dataset.mp4" type="video/mp4">
              </video>
            </div>
          </div>

          <div class="column is-half">
            <div class="content has-text-centered">
              <p>Offline Dataset \(\mathcal{D}_{prior}\)</p>
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/libero90_dataset.mp4" type="video/mp4">
              </video>
            </div>
          </div>
        </div>


      </div>

    </div>
    </div>
  </section>
  <!--/ Experiments -->

  <!-- Qualitative DTW -->
  <!-- <section class="section"> -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">

          <h2 class="title is-3">Qualitative Results</h2>
        </div>
      </div>

      <div class="container is-max-desktop">
        <br>
        <p>
        <h3 class="title is-5">Does \(\texttt{STRAP}\) retrieval scale to DROID?</h3>
        </p>
        <div class="columns is-mobile is-vcentered">
          <div class="column is-half">
            <div class="content has-text-centered">
              <p>Demonstration Dataset \(\mathcal{D}_{target}\)</p>
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/retrieval_droid/demo.mp4" type="video/mp4">
              </video>
              <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
            </div>
          </div>

          <div class="column is-half">
            <div class="content has-text-centered">
              <p>Data Retrieved from DROID \(\mathcal{D}_{retrieved}\)</p>
              <video id="replay-video" autoplay playsinline loop muted controls>
                <source src="./static/videos/retrieval_droid/retrieved.mp4" type="video/mp4">
              </video>
              <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
            </div>
          </div>
        </div>
        <br>
        <br>
        <p>
          We encode 5k random demonstrations from the DROID dataset and retrieve relevant sub-trajectories.
          \(\texttt{STRAP}\) finds <b>semantically relevant tasks from other lab's with similar environment and
            camera setups</b>.
          Note that our environment is not part of DROID!
        </p>
      </div>
      <br>
      <br>

      <div class="container is-max-desktop">
        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <br>
            <h3 class="title is-5">What types of matches are identified by <i>S-DTW</i>?</h3>
            <p>
              <b>Kitchen-DROID:</b><br>Below shows the demonstration segments (\(\mathcal{D}_{target}\)) denoted by
              the red box <span style="color: #DC1D64;"><b>&#9724;</b></span> and retrieved sub-trajectories (from \(\mathcal{D}_{prior}\)).
              \(\texttt{STRAP}\) <b>retrieves relevant segments</b> from the multi-task trajectories even if <b>object appearance and pose, or the task differ</b>.
              <br>
            </p>
            <div class="column is-full-width has-text-centered">
              <br>
              <p>Use the buttons <b>&larr;</b> <b>&rarr;</b> to explore all tasks!</p>
              <br>
              <button class="button" id="prev-button" onclick="prevRealTask()">
                <span class="icon">
                  <i class="fas fa-arrow-left"></i>
                </span>
                <span>Prev.</span>
              </button>
              <button class="button" id="next-button" onclick="nextRealTask()">
                <span>Next</span>
                <span class="icon">
                  <i class="fas fa-arrow-right"></i>
                </span>
              </button>
            </div>

            <div class="column has-text-centered">

              <p><b>Task Instruction:</b> <span id="task-real-string"><i>"pick up the pepper ü´ë and put it in the sink
                    üö∞"</i></span></p>
              <br>
              <div id="task-real-vid-input-0" class="rgb-image">
                <video id="task-real-vid" autoplay loop playsinline muted>
                  <source src="./static/videos/retrieval_strips/pepper_3/segment_0/video_strip.mp4" type="video/mp4">
              </div>

              <div id="task-real-vid-input-1" class="rgb-image">
                <video id="task-real-vid" autoplay loop playsinline muted>
                  <source src="./static/videos/retrieval_strips/pepper_3/segment_1/video_strip.mp4" type="video/mp4">
              </div>

              <div id="task-real-vid-input-2" class="rgb-image">
              </div>

              <div id="task-real-vid-input-3" class="rgb-image">
              </div>

              <div id="task-real-vid-input-4" class="rgb-image">
              </div>

              <div id="task-real-vid-input-5" class="rgb-image">
              </div>
              
              <div id="task-real-legend" class="rgb-image">
              <img src="./static/videos/retrieval_strips/pepper_3/legend.png" alt="color language mapping" 
              style="display: block; margin: 0 auto; max-width: 70%; height: auto;"/>
              </div>
            </div>
          </div>
        </div>

      </div>

      <div class="container is-max-desktop">
        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <!-- <br>
            <b>What types of matches are identified by <i>S-DTW</i>?</b>
            <br>
            <br> -->
            <br>
            <p>
              <b>LIBERO-10:</b><br>Below shows the demonstration segments (\(\mathcal{D}_{target}\)) denoted by
              the red box <span style="color: #DC1D64;"><b>&#9724;</b></span> and retrieved sub-trajectories (from \(\mathcal{D}_{prior}\)).
              \(\texttt{STRAP}\) <b>retrieves relevant segments</b> from the multi-task trajectories <b>matching the demonstrations</b>.
              <br>
            </p>
            <div class="column is-full-width has-text-centered">
              <br>
              <p>Use the buttons <b>&larr;</b> <b>&rarr;</b> to explore all tasks!</p>
              <br>
              <button class="button" id="prev-button" onclick="prevLiberoTask()">
                <span class="icon">
                  <i class="fas fa-arrow-left"></i>
                </span>
                <span>Prev.</span>
              </button>
              <button class="button" id="next-button" onclick="nextLiberoTask()">
                <span>Next</span>
                <span class="icon">
                  <i class="fas fa-arrow-right"></i>
                </span>
              </button>
            </div>

            <div class="column has-text-centered">

              <p><b>Task Instruction:</b> <span id="task-libero-string"><i>"pick up the book and place it in the back
                    compartment of the caddy"</i></span></p>
              <br>
              <div id="task-libero-vid-input-0" class="rgb-image">
                <video id="task-libero-vid" autoplay loop playsinline muted>
                  <source
                    src="./static/videos/retrieval_strips/pick_up_the_book_and_place_it_in_the_back_compartment_of_the_caddy/segment_0/video_strip.mp4"
                    type="video/mp4">
              </div>

              <div id="task-libero-vid-input-1" class="rgb-image">
                <video id="task-libero-vid" autoplay loop playsinline muted>
                  <source
                    src="./static/videos/retrieval_strips/pick_up_the_book_and_place_it_in_the_back_compartment_of_the_caddy/segment_1/video_strip.mp4"
                    type="video/mp4">
              </div>

              <div id="task-libero-vid-input-2" class="rgb-image">
                <video id="task-libero-vid" autoplay loop playsinline muted>
                  <source
                    src="./static/videos/retrieval_strips/pick_up_the_book_and_place_it_in_the_back_compartment_of_the_caddy/segment_2/video_strip.mp4"
                    type="video/mp4">
              </div>

              <div id="task-libero-vid-input-3" class="rgb-image">
                <video id="task-libero-vid" autoplay loop playsinline muted>
                  <source
                    src="./static/videos/retrieval_strips/pick_up_the_book_and_place_it_in_the_back_compartment_of_the_caddy/segment_3/video_strip.mp4"
                    type="video/mp4">
              </div>

              <div id="task-libero-vid-input-4" class="rgb-image">
                <!-- <video id="task-libero-vid" autoplay loop playsinline muted>
              <source src="./static/videos/video_strip (1).mp4" type="video/mp4"> -->
              </div>

              <div id="task-libero-vid-input-5" class="rgb-image">
                <!-- <video id="task-libero-vid" autoplay loop playsinline muted>
              <source src="./static/videos/video_strip (1).mp4" type="video/mp4"> -->
              </div>

              <div id="task-libero-vid-input-6" class="rgb-image">
                <!-- <video id="task-libero-vid" autoplay loop playsinline muted>
              <source src="./static/videos/video_strip (1).mp4" type="video/mp4"> -->
              </div>

              <div id="task-libero-vid-input-7" class="rgb-image">
                <!-- <video id="task-libero-vid" autoplay loop playsinline muted>
              <source src="./static/videos/video_strip (1).mp4" type="video/mp4"> -->
              </div>

              <div id="task-libero-vid-input-8" class="rgb-image">
                <!-- <video id="task-libero-vid" autoplay loop playsinline muted>
              <source src="./static/videos/video_strip (1).mp4" type="video/mp4"> -->
              </div>

              <div id="task-libero-vid-input-9" class="rgb-image">
                <!-- <video id="task-libero-vid" autoplay loop playsinline muted>
              <source src="./static/videos/video_strip (1).mp4" type="video/mp4"> -->
              </div>

            </div>
          </div>
        </div>

      </div>

      <br>
      <br>
      <div class="container is-max-desktop">
        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">

            <h3 class="title is-5">How does \(\texttt{STRAP}\) compare to other retrieval mechanisms?</h3>
            <br>
            <p>
            We prompt all retrieval methods with demonstrations for <i>"put the black bowl in the bottom drawer of the cabinet and close it"</i> and investigate from which tasks they retrieve their data. The figure shows the top-5 tasks and aggregates the remaining.
          </p>
            <br>
            <img src="./static/images/data_retrieved.png" class="money-figure" alt="Pipeline Overview" />
            <br>
            <br>

            <p>
              <b>\(\texttt{STRAP}\) only retrieves semantically relevant sub-trajectories</b> &mdash; each
              task
              shares at
              least one sub-task with the target task! For example, <i>"put the black bowl in the bottom
                drawer of
                the
                cabinet"</i>, <i>"close the bottom drawer of the cabinet ..."</i>. \(\texttt{STRAP}\)
              retrieval is
              sparse, only selecting data from 5/90 semantically relevant tasks and ignoring irrelevant ones
              (no
              <i>"others"</i> )!
            </p>
          </div>
        </div>
      </div>

    </div>
  </section>
  <!--/ Qualitative DTW -->

  <!-- Ablations -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Ablations</h2>
        </div>
      </div>
      <div class="container is-max-desktop">

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <p>
            <h3 class="title is-5">Does <i>sub-trajectory retrieval</i> improve performance in few-shot imitation
              learning?</h3>
            </p>
            <img src="./static/images/ablation_traj.png" class="money-figure" alt="Pipeline Overview"
              style="display: block; margin: 0 auto; max-width: 70%; height: auto;" />
            <br>
            <br>
            <p>
              We compare sub-trajectory retrieval with S-DTW (\(\texttt{STRAP}\)) to retrieving full trajectories
              (D-T)
              and states (D-S). <b>We find sub-trajectory retrieval to be preferred over states and full
                trajectories.</b> Full trajectories can contain segments irrelevant to the task, effectively hurting
              performance and reducing the accuracy of S-DTW.
            </p>
          </div>
        </div>
      </div>
      <br>
      <br>
      <div class="container is-max-desktop">
        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <p>
            <h3 class="title is-5">How effective are the representations from <i>vision-foundation models</i> for
              retrieval?</h3>
            </p>
            <img src="./static/images/ablation_representation.png" class="money-figure" alt="Pipeline Overview"
              style="display: block; margin: 0 auto; max-width: 70%; height: auto;" />
            <br>
            <br>
            <p>
              We replace the in-domain feature extractors from <i>BehaviorRetrieval</i> (BR) [Du et al. 2023] and
              <i>FlowRetrieval</i> (FR) [Lin et al. 2024] trained on \(\mathcal{D}_{prior}\) with an off-the-shelf
              DINOv2 encoder model (D-S). The choice of representation depends on the task with no method
              outperforming the others on all tasks.
              We want to highlight that <b>vision foundation models don't have to be trained on
                \(\mathcal{D}_{prior}\) and scale much better</b> with increasing amounts of trajectory data and
              on
              unseen tasks.
            </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered is-mobile is-vcentered">

      </div>
    </div>
  </section>
  <!--/ Ablations -->

  <!-- DTW -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Subsequence Dynamic Time Warping</h2>
        </div>
      </div>
      <br>
      <div class="container is-max-desktop">

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">

            <img src="./static/images/dtw_matching.png" class="money-figure" alt="Pipeline Overview" />
            <br>
            <br>
            <p>
              <b>Subsequence Dynamic Time Warping:</b>

              To retrieve sequences of variable length, we build Dynamic Time Warping (DTW). DTW methods compute the
              similarity between two sequence that may vary in length or frequency.
              The algorithm aligns the sequences by warping the time axis of the series using a set of step sizes to
              minimize the distance between corresponding points while obeying boundary conditions.
              Subsequence Dynamic Time Warping (S-DTW) loosens these boundary conditions to allow for subsequence
              matching.
            </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered is-mobile is-vcentered">

        <div class="container is-max-desktop">

          <div class="columns is-centered is-mobile is-vcentered">
            <div class="column is-full-width">
              <br>
              <br>
              <img src="./static/images/dtw_matrix.png" class="money-figure" alt="Pipeline Overview" />
              <br>
              <br>
              <p>
                DTW computes a <b>distance matrix</b> that compares the distance between each pair of points in the two
                sequences. We use vision foundation models, <i>e.g.</i>, DINOv2, to encode the image observations and
                compute their distance with the L2 norm. The optimal matching is represented by the <b>shortest path</b>
                through the matrix. Dynamic programming finds this path, minimizing the total distance between the two
                sequences.
                The figure above shows matching a demonstration from \(\mathcal{D}_{target}\) (y-axis) to a sub-sequence
                in the offline dataset \(\mathcal{D}_{prior}\) (x-axis). Brighter colors indicate higher <span style="color: 
  #fce624;"><b>&#9650;</b></span> and darker colors indicate lower <span style="color: 
  #440055;"><b>&#9660;</b></span> cost. The red line <span style="color: #e90a12;"><b>&#9644;</b></span> indicates the
                optimal path through the matrix, <i>i.e.</i>, the optimal match between the sequences.
              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered is-mobile is-vcentered">

        </div>
      </div>
    </div>
    </div>
  </section>
  <!--/ DTW -->

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{memmelstrap,
        title={STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning},
        author={Memmel, Marius and Berg, Jacob and Chen, Bingqing and Gupta, Abhishek and Francis, Jonathan},
        booktitle={1st Workshop on X-Embodiment Robot Learning}
      }
</code></pre>
    </div>
  </section>
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is based on the <a href="https://github.com/nerfies/nerfies.github.io"
                target="_blank">Nerfies website template</a>,
              which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              <a href="./static/videos/brain.mp4">üöÆ</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>