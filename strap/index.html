<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning">
  <meta name="keywords" content="Robot Learning, Retrieval, Dynamic Time Warping, Vision Foundation Models">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning</title>

  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XHJWDHYL2Z"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-XHJWDHYL2Z');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.ico">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <!-- TEX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js" integrity="sha384-..."
    crossorigin="anonymous"></script>
</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img style="vertical-align: middle; width: 250px; height: auto;" src="./static/images/strap_logo.png"
              alt="80s style ASID logo" />
            <h1 class="title is-1 publication-title">Robot Sub-Trajectory Retrieval for Augmented Policy Learning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://memmelma.github.io">Marius Memmel</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/jacob-b-066234203/">Jacob Berg</a><sup>*</sup><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=LYt_2MgAAAAJ&hl=en">Bingqing
                  Chen</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://abhishekunique.github.io/">Abhishek Gupta</a><sup>&dagger;1</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=7CLS0LwAAAAJ&hl=en">Jonathan
                  Francis</a><sup>&dagger;2</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>University of Washington</span>
              <span class="author-block"><sup>2</sup>Bosch Center for Artificial Intelligence</span>
            </div>
            <div>
              <span class="author-block"><sup>*</sup>equal contribution</span>
              <span class="author-block"><sup>&dagger;</sup>equal advising</span>
            </div>

            <!-- <div class="is-size-5 has-text-weight-semibold mt-3 publication-title">
              <span class="author-block">conference placeholder</span>
            </div> -->

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="static/documents/strap_2025.pdf" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="TBD" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv (coming soon)</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="YOUTUBE" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video (coming soon)</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="TBD" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (coming soon)</span>
                  </a>
                </span>

                <!-- Poster download link. -->
                <!-- <span class="link-block">
                  <a href="static/assets/CVPR2023_VOT_poster.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-sticky-note"></i>
                    </span>
                    <span>Poster</span>
                  </a>
                </span> -->
                <!-- Slides download link. -->
                <!-- <span class="link-block">
                  <a href="static/assets/CVPR2023_VOT_slides.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-powerpoint"></i>
                    </span>
                    <span>Slides</span>
                  </a>
                </span> -->
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <!-- <section class="hero teaser">
    <div class="container is-centered is-max-desktop">
      <div class="hero-body">
        <video id="teaser" autoplay muted loop playsinline>
          <source src="./static/videos/results_video.mp4" type="video/mp4">
        </video>
        <h2 class="subtitle has-text-centered">
          ASID is a generic pipeline for Sim2Real transfer that solves dynamic tasks zero-shot!
        </h2>
      </div>
  </section> -->

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <div class="content">

            <h2 class="title is-3">Abstract</h2>
            <div class="content has-text-justified">
              <p>
                Robot learning is witnessing a significant increase in the size, diversity, and complexity of
                pre-collected datasets, mirroring trends in domains such as natural language processing and computer
                vision. Many robot learning methods treat such datasets as multi-task expert data and learn a
                multi-task, generalist policy by training broadly across them. Notably, while these generalist policies
                can improve the average performance across many tasks, the performance of generalist policies on any one
                task is often suboptimal due to negative transfer between partitions of the data, compared to
                task-specific specialist policies. In this work, we argue for the paradigm of training policies during
                deployment given the scenarios they encounter: rather than deploying pre-trained policies to unseen
                problems in a zero-shot manner, we non-parametrically retrieve and train models directly on relevant
                data at test time. Furthermore, we show that many robotics tasks share considerable amounts of low-level
                behaviors and that retrieval at the "sub"-trajectory granularity enables significantly improved data
                utilization, generalization, and robustness in adapting policies to novel problems. In contrast,
                existing full-trajectory retrieval methods tend to underutilize the data and miss out on shared
                cross-task content. This work proposes \(\texttt{STRAP}\), a technique for leveraging pre-trained vision
                foundation
                models and dynamic time warping to retrieve sub-sequences of trajectories from large training corpora in
                a robust fashion. \(\texttt{STRAP}\) outperforms both prior retrieval algorithms and multi-task learning
                methods in
                simulated and real experiments, showing the ability to scale to much larger offline datasets in the real
                world as well as the ability to learn robust control policies with just a handful of real-world
                demonstrations.
              </p>
              <!-- <video id="teaser" autoplay muted playsinline loop>
                <source src="./static/videos/figure_1_animated_rod.mp4" type="video/mp4">
              </video> -->
            </div>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->

      <!-- Paper video. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Video</h2>
          <div class="publication-video">
            <iframe width="560" height="315" src="static/videos/strap_talk_crop.mp4" title="YouTube video player"
              frameborder="0"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              allowfullscreen></iframe>
          </div>
        </div>
      </div>
      <!--/ Paper video. -->
    </div>
  </section>

  <!-- Pipeline Overview -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Pipeline Overview</h2>
        </div>
      </div>
      <br>
      <div class="container is-max-desktop">

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">

            <img src="./static/images/method_figure.png" class="money-figure" alt="Pipeline Overview" />
            <p>
              <b>Overview of \(\texttt{STRAP}\):</b> 1) demonstrations \(\mathcal{D}_{target}\) and offline dataset
              \(\mathcal{D}_{prior}\) are encoded into a shared embedding space using a vision foundation model, 2)
              automatic slicing generates sub-trajectories which 3) Subsequence Dynamic Time Warping (S-DTW) matches to
              corresponding sub-trajectories in \(\mathcal{D}_{prior}\) creating \(\mathcal{D}_{retrieved}\), 4)
              training a policy on the union of \(\mathcal{D}_{retrieved}\) and \(\mathcal{D}_{target}\) results in
              better performance and robustness.
            </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered is-mobile is-vcentered">

      </div>
    </div>
    </div>
    </div>
  </section>
  <!--/ Pipeline Overview -->

  <!-- DTW -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Subsequence Dynamic Time Warping</h2>
        </div>
      </div>
      <br>
      <div class="container is-max-desktop">

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">

            <img src="./static/images/dtw_matching.png" class="money-figure" alt="Pipeline Overview" />
            <br>
            <br>
            <p>
              <b>Subsequence Dynamic Time Warping:</b>

              To retrieve sequences of variable length, we build Dynamic Time Warping (DTW). DTW methods compute the similarity between two sequence that may vary in length or frequency.
              The algorithm aligns the sequences by warping the time axis of the series using a set of step sizes to minimize the distance between corresponding points while obeying boundary conditions. 
              Subsequence Dynamic Time Warping (S-DTW) loosens these boundary conditions to allow for subsequence matching.
            </p>
          </div>
        </div>
      </div>
      <div class="columns is-centered is-mobile is-vcentered">

        <div class="container is-max-desktop">

          <div class="columns is-centered is-mobile is-vcentered">
            <div class="column is-full-width">
              <br>
              <br>
              <img src="./static/images/dtw_matrix.png" class="money-figure" alt="Pipeline Overview" />
              <br>
              <br>
              <p>
                DTW computes a <b>distance matrix</b> that compares the distance between each pair of points in the two sequences. We use vision foundation models, <i>e.g.</i>, DINOv2, to encode the image observations and compute their distance with the L2 norm. The optimal matching is represented by the <b>shortest path</b> through the matrix. Dynamic programming finds this path, minimizing the total distance between the two sequences.
                The figure above shows matching a demonstration from \(\mathcal{D}_{target}\) (y-axis) to a sub-sequence in the offline dataset \(\mathcal{D}_{prior}\) (x-axis). Brighter colors indicate higher <span style="color: 
#fce624;"><b>&#9650;</b></span> and darker colors indicate lower <span style="color: 
#440055;"><b>&#9660;</b></span> cost. The red line <span style="color: #e90a12;"><b>&#9644;</b></span> indicates the optimal path through the matrix, <i>i.e.</i>, the optimal match between the sequences.
              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered is-mobile is-vcentered">

        </div>
      </div>
    </div>
    </div>
  </section>
  <!--/ DTW -->

  <!-- Experiments -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Experiments</h2>
        </div>
      </div>
      <br>
      <div class="container is-max-desktop">

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">

            <p>
              <b>Simulation Experiments:</b>
            </p>
            <br>
            <br>

            <img src="./static/images/results_sim.png" class="money-figure" alt="Pipeline Overview"
              style="display: block; margin: 0 auto; max-width: 90%; height: auto;" />
            <br>
            <br>
            <p>
              <b>\(\texttt{STRAP}\) outperforms the retrieval baselines</b> <i>BehaviorRetrieval</i> (BR) [Du et al.
              2023] and
              <i>FlowRetrieval</i> (FR) [Lin et al. 2024] on average by \(+12.20\%\) and \(+12.47\%\) across all
              LIBERO-10 tasks. These results demonstrate the policy's robustness to object poses. Both <b>DINOv2
                and CLIP are viable representations for \(\texttt{STRAP}\)</b> with only a
              \(+0.73\%\)
              difference across all LIBERO-10 tasks.
            </p>
          </div>
        </div>
      </div>
      <br>
      <br>
      <div class="columns is-centered is-mobile is-vcentered">

        <br>
        <div class="container is-max-desktop">

          <div class="columns is-centered is-mobile is-vcentered">
            <div class="column is-full-width">

              <p>
                <b>Real-World Experiments:</b>
              </p>
              <br>
              <br>

              <img src="./static/images/results_real.png" class="money-figure" alt="Pipeline Overview"
                style="display: block; margin: 0 auto; max-width: 50%; height: auto;" />
              <br>
              <p>
                While Behavior Cloning (BC) and \(\texttt{STRAP}\) solve the Franka-Pen-in-Cup demonstrated in \(\mathcal{D}_{target}\)
                (<i>base</i>), BC lacks robustness to out-of-distribution (<i>OOD</i>) scenarios. The policy replays
                the trajectories observed in \(\mathcal{D}_{target}\). \(\texttt{STRAP}\) retrieves relevant sub-trajectories from \(\mathcal{D}_{prior}\), <i>e.g.</i>,
                the robot putting the screwdriver in the cup or picking up pens in various poses. Augmented policy
                learning then distills this knowledge into a policy, resulting in generalization to an OOD scenario.
              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered is-mobile is-vcentered">

        </div>
      </div>
    </div>
    </div>
  </section>
  <!--/ Experiments -->

  <!-- Ablations -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Ablations</h2>
        </div>
      </div>
      <br>
      <div class="container is-max-desktop">

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <p>
              <b>Does <i>sub-trajectory retrieval</i> improve performance in few-shot imitation learning?</b>
            </p>
            <br>
            <br><img src="./static/images/ablation_traj.png" class="money-figure" alt="Pipeline Overview"
              style="display: block; margin: 0 auto; max-width: 70%; height: auto;" />
            <br>
            <br>
            <p>
              We compare sub-trajectory retrieval with S-DTW (\(\texttt{STRAP}\)) to retrieving full trajectories (D-T)
              and states (D-S). <b>We find sub-trajectory retrieval to be preferred over states and full
                trajectories.</b> Full trajectories can contain segments irrelevant to the task, effectively hurting
              performance and reducing the accuracy of S-DTW.
            </p>
          </div>
        </div>
      </div>
      <br>
      <br>
      <div class="columns is-centered is-mobile is-vcentered">

        <br>
        <div class="container is-max-desktop">

          <div class="columns is-centered is-mobile is-vcentered">
            <div class="column is-full-width">
              <p>
                <b>How effective are the representations from <i>vision-foundation models</i> for retrieval?</b>
              </p>
              <br>
              <br>

              <img src="./static/images/ablation_representation.png" class="money-figure" alt="Pipeline Overview"
                style="display: block; margin: 0 auto; max-width: 70%; height: auto;" />
              <br>
              <br>
              <p>
                We replace the in-domain feature extractors from <i>BehaviorRetrieval</i> (BR) [Du et al. 2023] and
                <i>FlowRetrieval</i> (FR) [Lin et al. 2024] trained on \(\mathcal{D}_{prior}\) with an off-the-shelf
                DINOv2 encoder model (D-S). The choice of representation depends on the task with no method
                outperforming the others on all tasks.
                We want to highlight that <b>vision foundation models don't have to be trained on
                  \(\mathcal{D}_{prior}\) and scale much better</b> with increasing amounts of trajectory data and on
                unseen tasks.
              </p>
            </div>
          </div>
        </div>
        <div class="columns is-centered is-mobile is-vcentered">

        </div>
      </div>
    </div>
    </div>
  </section>
  <!--/ Ablations -->

  <!-- Real-World Experiments -->
  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered is-mobile is-vcentered">
        <div class="column is-full-width">
          <h2 class="title is-3">Real-World Experiments</h2>
        </div>
      </div>
      <br>
      <!-- Intuition -->
      <div class="container is-max-desktop">
        <h2 class="title is-4">Pen-in-Cup (base)</h2>

        <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <div class="content has-text-centered">
              <video id="replay-video" controls autoplay playsinline loop muted>
                <source src="./static/videos/pen_in_cup_base.mp4" type="video/mp4">
              </video>
              <!-- <h4 class="title is-8">Pen-in-Cup (base)</h4> -->
            </div>
          </div>
        </div>
        <!-- <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <p>
              To identify the underlying physics parameters that govern the dynamics of the system, we need to collect
              trajectories that are strongly affected by these parameters. In the case of the rod balancing task, the
              center of mass affects how the rod rotates when pushed by the robot during the exploration
              stage. As shown above, the same actions can lead to drastically different rod poses after interaction.
              Trajectories are
              uninformative if the rod is not forced to rotate around its center of mass or doesn't
              move at all. We indicate the center of mass by a blue marker <span
                style="color: #0000ff; font-size: 15px;">●</span>
              on top of the rod in both sim and real (not visible to the policy).
            </p>
          </div>
        </div> -->

        <div class="container is-max-desktop">
          <h2 class="title is-4">Pen-in-Cup (out-of-distribution)</h2>

          <div class="columns is-centered is-mobile is-vcentered">
            <div class="column is-full-width">
              <div class="content has-text-centered">
                <video id="replay-video" controls autoplay playsinline loop muted>
                  <source src="./static/videos/pen_in_cup_ood.mp4" type="video/mp4">
                </video>
                <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
              </div>
            </div>
          </div>

          <div class="container is-max-desktop">
            <h2 class="title is-4">Datasets</h2>
  
            <div class="columns is-mobile is-vcentered">
              <div class="column is-half">
                <div class="content has-text-centered">
                  <p>Demonstration Dataset \(\mathcal{D}_{target}\)</p>
                  <video id="replay-video" autoplay playsinline loop muted controls>
                    <source src="./static/videos/pick_dataset.mp4" type="video/mp4">
                  </video>
                  <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
                </div>
              </div>
            
              <div class="column is-half">
                <div class="content has-text-centered">
                  <p>Offline Dataset \(\mathcal{D}_{prior}\)</p>
                  <video id="replay-video" autoplay playsinline loop muted controls>
                    <source src="./static/videos/10x10_dataset.mp4" type="video/mp4">
                  </video>
                  <!-- <h4 class="title is-8">Pen-in-Cup (out-of-distribution)</h4> -->
                </div>
              </div>
            </div>
          <!-- <div class="columns is-centered is-mobile is-vcentered">
          <div class="column is-full-width">
            <p>
              To identify the underlying physics parameters that govern the dynamics of the system, we need to collect
              trajectories that are strongly affected by these parameters. In the case of the rod balancing task, the
              center of mass affects how the rod rotates when pushed by the robot during the exploration
              stage. As shown above, the same actions can lead to drastically different rod poses after interaction.
              Trajectories are
              uninformative if the rod is not forced to rotate around its center of mass or doesn't
              move at all. We indicate the center of mass by a blue marker <span
                style="color: #0000ff; font-size: 15px;">●</span>
              on top of the rod in both sim and real (not visible to the policy).
            </p>
          </div>
        </div> -->


        </div>
        <!-- Real-World Experiments -->

        <!-- Qualitative DTW -->
        <section class="section">
          <div class="container is-max-desktop">

            <div class="columns is-centered is-mobile is-vcentered">
              <div class="column is-full-width">
                <h2 class="title is-3">Qualitative Results</h2>
              </div>
            </div>
            <div class="container is-max-desktop">

              <div class="columns is-centered is-mobile is-vcentered">
                <div class="column is-full-width">

                  <b>What types of matches are identified by <i>S-DTW</i>?</b>
                  <br>
                  <br>
                  <img src="./static/images/data_retrieved.png" class="money-figure" alt="Pipeline Overview" />
                  <br>
                  <br>
                  <p>
                    We visualize the top five tasks retrieved and accumulates the rest into the <i>"others"</i>
                    category.
                    <b>\(\texttt{STRAP}\) only retrieves semantically relevant sub-trajectories</b> &mdash; each task
                    shares at
                    least one sub-task with the target task! For example, <i>"put the black bowl in the bottom drawer of
                      the
                      cabinet"</i>, <i>"close the bottom drawer of the cabinet ..."</i>. \(\texttt{STRAP}\) retrieval is
                    sparse, only selecting data from 5/90 semantically relevant tasks and ignoring irrelevant ones (no
                    <i>"others"</i>)!
                  </p>
                </div>
              </div>
            </div>
            <div class="columns is-centered is-mobile is-vcentered">

            </div>
          </div>
      </div>
    </div>
  </section>
  <!--/ Qualitative DTW -->

  <br>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{2025strap,
  title={???},
  author={???},
  journal={???},
  year={2025}
}
</code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is based on the <a href="https://github.com/nerfies/nerfies.github.io"
                target="_blank">Nerfies website template</a>,
              which is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"
                target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>